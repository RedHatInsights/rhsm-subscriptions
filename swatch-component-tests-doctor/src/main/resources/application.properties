# Use the bge-small-en-q embedding model:
quarkus.langchain4j.embedding-model.provider=dev.langchain4j.model.embedding.onnx.bgesmallenq.BgeSmallEnQuantizedEmbeddingModel

# File extensions to ingest (comma-separated)
ingest.allowed-extensions=.java,.kt,.xml,.yml,.yaml,.json,.properties,.md,.txt,.sql,.sh

# Maximum file size to ingest (in bytes, default 5MB)
ingest.max-file-size=5242880

# Minimum file size to ingest (in bytes, default 10 bytes to skip empty/blank files)
ingest.min-file-size=10

# Directories to skip during ingestion (comma-separated)
ingest.skip-directories=target,build,node_modules,.git,.idea,dist,out,.gradle,.mvn,bin

# Document chunking configuration
ingest.chunk-size=500
ingest.chunk-overlap=50

# To simplify deployment and development, we'll package the server as an uber-jar.
# This makes it possible to mvn install and publish as a jar
quarkus.package.jar.type=uber-jar

# ==============================================================================
# Datasource Configuration
# ==============================================================================

# Default datasource: pgvector (for embeddings and metadata)
quarkus.datasource.db-kind=postgresql
quarkus.datasource.username=doctor
quarkus.datasource.password=doctor
quarkus.datasource.jdbc.url=jdbc:postgresql://localhost:5433/doctor

# Named datasource: rhsm-subscriptions (for querying test database)
quarkus.datasource.rhsm-subscriptions.db-kind=postgresql
quarkus.datasource.rhsm-subscriptions.username=rhsm-subscriptions
quarkus.datasource.rhsm-subscriptions.password=rhsm-subscriptions
quarkus.datasource.rhsm-subscriptions.jdbc.url=jdbc:postgresql://localhost:5432/rhsm-subscriptions

# Hibernate configuration (uses default datasource for metadata)
quarkus.hibernate-orm.database.default-schema=public
quarkus.hibernate-orm.log.sql=false
quarkus.hibernate-orm.log.jdbc-warnings=false
quarkus.hibernate-orm.schema-management.strategy=update

# Size of the embeddings that will be stored:
quarkus.langchain4j.pgvector.dimension=384

# ==============================================================================
# AI Model Configuration
# ==============================================================================
# Both Gemini and Ollama are included. Choose at runtime with -Dquarkus.profile
# 
# 1. Gemini (default) - Free tier with 1500 requests/day
#    Run: java -jar target/*-runner.jar investigate <test>
#    Requires: export GEMINI_API_KEY="AIza..."
#
# 2. Ollama (local) - No API costs, runs locally
#    Run: docker-compose up -d ollama
#         docker exec -it swatch-component-tests-ollama-1 ollama pull llama3.2:1b
#         java -Dquarkus.profile=ollama -jar target/*-runner.jar investigate <test>
#
# Profile-specific configuration is in:
# - application-gemini.properties (default)
# - application-ollama.properties

# Default profile is Gemini
%dev.quarkus.profile=gemini
%prod.quarkus.profile=gemini

# Logging configuration (optional, for debugging)
quarkus.log.category."com.redhat.swatch.component.tests.doctor".level=INFO
