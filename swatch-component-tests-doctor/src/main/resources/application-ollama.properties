# ==============================================================================
# AI Model Configuration - Ollama (Local inference)
# ==============================================================================
# Ollama provides local LLM inference without API costs
# Start with: docker-compose up -d ollama
# Pull model: docker exec -it swatch-component-tests-ollama-1 ollama pull mistral
# 
# Recommended models for function calling (2025):
# - mistral (RECOMMENDED - native function calling support, 4GB, excellent for tools)
# - deepseek-r1:7b (strong reasoning, 4-5GB)
# - qwen2.5:7b (multilingual, 32K context, 4GB)
# - llama3.1:8b (good quality, 4.7GB)

# Select Ollama as the chat model provider
quarkus.langchain4j.chat-model.provider=ollama

quarkus.langchain4j.ollama.base-url=http://localhost:11434
quarkus.langchain4j.ollama.timeout=300s
quarkus.langchain4j.ollama.log-requests=false
quarkus.langchain4j.ollama.log-responses=false

# Chat model configuration
quarkus.langchain4j.ollama.chat-model.model-id=mistral
quarkus.langchain4j.ollama.chat-model.temperature=0.1

# Note: Ollama runs locally, no API key needed
# To use: mvn clean package -Pollama
#         java -Dquarkus.profile=ollama -jar target/*-runner.jar investigate <test>

