---
version: '3.8'
services:
  nginx:
    image: docker.io/library/nginx:latest
    ports:
      - "8000:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:z
  wiremock:
    extends:
      file: config/wiremock/docker-compose.yml
      service: wiremock
    volumes:
      - ./config/wiremock/__files:/home/wiremock/__files:z
      - ./config/wiremock/mappings:/home/wiremock/mappings:z
    ports:
      - "127.0.0.1:8006:8080"
    networks:
      swatch-network:
        aliases:
          - wiremock
  prometheus:
    extends:
      file: config/prometheus/docker-compose.yml
      service: prometheus
    networks:
      swatch-network:
        aliases:
          - prometheus
  amqp:
    extends:
      file: config/artemis/docker-compose.yml
      service: artemis
    networks:
      swatch-network:
        aliases:
          - amqp
  db:
    # Use the same version as in https://gitlab.cee.redhat.com/service/app-interface/-/blob/ff61d457898da76ebd4abf21fe3ce7b5c74c87a5/data/services/insights/rhsm/namespaces/rhsm-prod.yml#L179
    # When updating the image, remember to also update the following locations:
    # - SwatchPostgreSQLContainer.POSTGRESQL_IMAGE
    # - .github/workflows/pr.yaml (step "Setup Postgresql Database")
    image: quay.io/sclorg/postgresql-13-c9s:c9s
    environment:
      - POSTGRES_HOST_AUTH_METHOD=trust
      - POSTGRESQL_MAX_CONNECTIONS=5000
      - POSTGRESQL_ADMIN_PASSWORD=admin
    healthcheck:
      test: ["CMD", "pg_isready", "--username=postgres", "--host=127.0.0.1", "--port=5432"]
      interval: 2s
      timeout: 1m
      retries: 5
      start_period: 10s
    volumes:
      - ./init_dbs.sh:/usr/share/container-scripts/postgresql/init/init_dbs.sh:z
      - ./postgresql.conf:/opt/app-root/src/postgresql-cfg/postgresql.conf:z
      - ./pg_hba.conf:/pg_hba.conf:z
    ports:
      - "127.0.0.1:5432:5432"
    networks:
      swatch-network:
        aliases:
          - db
  kafka:
    container_name: swatch-kafka
    image: quay.io/strimzi/kafka:latest-kafka-3.1.0
    command: sh /init_kafka.sh
    environment:
      # Enable SSL debugging
      # - KAFKA_OPTS=-Djavax.net.debug=ssl,handshake,data,trustmanager,keymanager
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT,SASL_SSL:SASL_SSL
      - KAFKA_LISTENERS=PLAINTEXT://:29092,PLAINTEXT_HOST://:9092,CONTROLLER://:9093,SASL_SSL://:9094
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092,SASL_SSL://localhost:9094
      - LOG_DIR=/tmp/logs
      - KAFKA_SSL_CLIENT_AUTH=required
      - KAFKA_SSL_KEYSTORE_LOCATION=/etc/kafka/secrets/certs/server.jks
      - KAFKA_SSL_KEYSTORE_PASSWORD=password
      - KAFKA_SSL_TRUSTSTORE_LOCATION=/etc/kafka/secrets/certs/test-ca.jks
      - KAFKA_SSL_TRUSTSTORE_PASSWORD=password
        # Passed directly to the JVM. Place SSL debug options here (e.g. -Djavax.net.debug=ssl)
      - KAFKA_OPTS=-Djava.security.auth.login.config=/etc/kafka/kafka_server_jaas.conf
        # Note that as of this writing (20 Jul 2022), Kraft mode does not support SCRAM
      - KAFKA_SASL_MECHANISM=PLAIN
    healthcheck:
      test: ./bin/kafka-cluster.sh cluster-id --bootstrap-server 127.0.0.1:9092 || exit 1
      interval: 2s
      timeout: 1m
      retries: 5
      start_period: 10s
    ports:
      - "127.0.0.1:9092:9092"
      - "[::1]:9092:9092"
      # Port 9093 is used by the Kraft configuration
      - "127.0.0.1:9094:9094"
      - "127.0.0.1:29092:29092"
    volumes:
      - ./config/kafka/init_kafka.sh:/init_kafka.sh:z
      - ./config/kafka/kafka_server_jaas.conf:/etc/kafka/kafka_server_jaas.conf:z
      - ./config/kafka/:/etc/kafka/secrets/certs:z
    networks:
      swatch-network:
        aliases:
          - kafka
    user: root
  kafka-rest:
    image: docker.io/confluentinc/cp-kafka-rest
    environment:
      - KAFKA_REST_BOOTSTRAP_SERVERS=kafka:29092
    depends_on:
      kafka:
        condition: service_healthy
  kafka-setup:
    image: quay.io/strimzi/kafka:latest-kafka-3.1.0
    command: |
      /bin/bash -c "
        bin/kafka-topics.sh --bootstrap-server=kafka:29092 --create --if-not-exists --partitions 1 --replication-factor 1 --topic platform.inventory.host-ingress
        bin/kafka-topics.sh --bootstrap-server=kafka:29092 --create --if-not-exists --partitions 1 --replication-factor 1 --topic platform.inventory.events
        bin/kafka-topics.sh --bootstrap-server=kafka:29092 --create --if-not-exists --partitions 1 --replication-factor 1 --topic platform.notifications.ingress
        bin/kafka-topics.sh --bootstrap-server=kafka:29092 --create --if-not-exists --partitions 1 --replication-factor 1 --topic platform.rhsm-subscriptions.service-instance-ingress
        bin/kafka-topics.sh --bootstrap-server=kafka:29092 --create --if-not-exists --partitions 1 --replication-factor 1 --topic platform.rhsm-subscriptions.billable-usage
        bin/kafka-topics.sh --bootstrap-server=kafka:29092 --create --if-not-exists --partitions 1 --replication-factor 1 --topic platform.rhsm-subscriptions.billable-usage-hourly-aggregate
      "
    depends_on:
      kafka:
        condition: service_healthy
  kafka-topics-ui:
    image: docker.io/landoop/kafka-topics-ui
    environment:
      - KAFKA_REST_PROXY_URL=http://kafka-rest:8082
      - PROXY=true
    ports:
      - "127.0.0.1:3030:8000"
    restart: on-failure
    depends_on:
      - kafka-rest
  kafka-bridge:
    image: quay.io/strimzi/kafka-bridge:0.31.2
    entrypoint: /opt/strimzi/bin/kafka_bridge_run.sh
    command: --config-file=config/application-kafka-bridge.properties
    ports:
      - "127.0.0.1:9080:8080"
    volumes:
      - ./config/application-kafka-bridge.properties:/opt/strimzi/config/application-kafka-bridge.properties:Z
    networks:
      - swatch-network
    user: root
    depends_on:
      kafka:
        condition: service_healthy
  inventory:
    image: quay.io/cloudservices/insights-inventory
    entrypoint: /bin/bash -c
    command: ["make upgrade_db && ./run_gunicorn.py"]
    environment:
      - INVENTORY_LOG_LEVEL=DEBUG
      - INVENTORY_DB_HOST=db
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - prometheus_multiproc_dir=/tmp
      - BYPASS_RBAC=true
      - FLASK_APP=./manage.py
      - UNLEASH_TOKEN=default:development.unleash-insecure-api-token
      - UNLEASH_URL=http://unleash:4242/api
      - UNLEASH_CACHE_DIR=/tmp/.unleashcache
    healthcheck:
      test: curl --fail http://localhost:8080/health
      interval: 2s
      timeout: 1m
      retries: 5
      start_period: 10s
    depends_on:
      kafka:
        condition: service_healthy
      db:
        condition: service_healthy
      unleash:
        condition: service_healthy
    restart: unless-stopped
    ports:
      - "127.0.0.1:8050:8000"
    user: root
  inventory-mq:
    image: quay.io/cloudservices/insights-inventory
    entrypoint: /bin/bash -c
    command: ["make run_inv_mq_service"]
    environment:
      - INVENTORY_LOG_LEVEL=DEBUG
      - INVENTORY_DB_HOST=db
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - prometheus_multiproc_dir=/tmp
      - BYPASS_RBAC=true
      - FLASK_APP=./manage.py
      - UNLEASH_TOKEN=default:development.unleash-insecure-api-token
      - UNLEASH_URL=http://unleash:4242
      - UNLEASH_CACHE_DIR=/tmp/.unleashcache
    depends_on:
      kafka:
        condition: service_healthy
      db:
        condition: service_healthy
      kafka-setup:
        condition: service_completed_successfully
      inventory:  # main inventory deployment runs the db migrations
        condition: service_healthy
    user: root
  unleash:
    image: quay.io/cloudservices/unleash-server:5.8.2
    environment:
      - INIT_CLIENT_API_TOKENS=default:development.unleash-insecure-api-token
      - INIT_ADMIN_API_TOKENS=*:*.unleash-insecure-admin-api-token
      - CHECK_VERSION=false
      - DATABASE_HOST=db
      - DATABASE_NAME=unleash
      - DATABASE_USERNAME=${DATABASE_USER:-unleash}
      - DATABASE_PASSWORD=${DATABASE_PASSWORD:-unleash}
      - DATABASE_SSL=false
      - IMPORT_DROP_BEFORE_IMPORT=false
      - IMPORT_FILE=/.unleash/flags.json
      - IMPORT_DROP_BEFORE_IMPORT=true
      - LOG_LEVEL=INFO
    healthcheck:
      test: wget --no-verbose --tries=1 --spider http://localhost:4242/health || exit 1
      interval: 2s
      timeout: 1m
      retries: 5
      start_period: 10s
    # Why do we do this?  Because there is an issue around the postgresql container where it
    # starts, runs some initialization scripts, and then restarts itself.  See
    # https://github.com/sclorg/postgresql-container/blob/master/12/root/usr/bin/run-postgresql#L54-L58
    # This fools the healthcheck into reporting that the container is ready, only for the
    # postgresql process to stop and restart.  Meanwhile, the unleash container tries to connect,
    # fails, and doesn't start.  So we add this shim script to wait until the connection is stable.
    # The script itself is sourced from the https://github.com/Unleash/unleash-docker container
    command: ["/bin/sh", "/unleash/wait-for", "db:5432", "--", "node", "index.js"]
    depends_on:
      db:
        condition: service_healthy
    ports:
      - "127.0.0.1:4242:4242"
    volumes:
      - './bin/wait-for:/unleash/wait-for:z'
      - './.unleash:/.unleash:z'
  moto:
    extends:
      file: config/moto/docker-compose.yml
      service: moto
    networks:
      swatch-network:
        aliases:
          - moto
networks:
  swatch-network:
    name: swatch-network
    driver: bridge
